{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ec7afe74-6b81-40ac-8378-a006da8bf4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.optim import AdamW\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForSequenceClassification,\n",
    "    AutoModelForSeq2SeqLM,\n",
    "    #AutoModelForSeq2SeqLM & AutoModelForCausalLM for text generation\n",
    "    AutoModelForCausalLM,\n",
    "    T5ForConditionalGeneration,\n",
    "    get_linear_schedule_with_warmup,\n",
    "    pipeline\n",
    ")\n",
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "from rouge_score import rouge_scorer\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "from tqdm import tqdm\n",
    "import json\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "7c2fa300-be0f-4c32-9471-9cce4121ed59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set seed for reproducibility\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "34aa622d-4718-4934-9a3b-534f766ecd6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    \"\"\"Dataset for text classification tasks\"\"\"\n",
    "    def __init__(self, texts, labels, tokenizer, max_len=128):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_len,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'label': torch.tensor(label, dtype=torch.long)\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9aa89cfe-a151-4de0-9314-cf85155d8477",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextSeq2SeqDataset(Dataset):\n",
    "    \"\"\"Dataset for sequence-to-sequence tasks (summarization, translation)\"\"\"\n",
    "    def __init__(self, source_texts, target_texts, tokenizer, max_source_len=512, max_target_len=128):\n",
    "        self.source_texts = source_texts\n",
    "        self.target_texts = target_texts\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_source_len = max_source_len\n",
    "        self.max_target_len = max_target_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.source_texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        source_text = str(self.source_texts[idx])\n",
    "        target_text = str(self.target_texts[idx])\n",
    "\n",
    "        source_encoding = self.tokenizer(\n",
    "            source_text,\n",
    "            max_length=self.max_source_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        target_encoding = self.tokenizer(\n",
    "            target_text,\n",
    "            max_length=self.max_target_len,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "\n",
    "        input_ids = source_encoding['input_ids'].flatten()\n",
    "        attention_mask = source_encoding['attention_mask'].flatten()\n",
    "        labels = target_encoding['input_ids'].flatten()\n",
    "\n",
    "        # Replace padding token id's with -100 so they are ignored in the loss\n",
    "        labels[labels == self.tokenizer.pad_token_id] = -100\n",
    "\n",
    "        return {\n",
    "            'input_ids': input_ids,\n",
    "            'attention_mask': attention_mask,\n",
    "            'labels': labels\n",
    "        }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "7b632ec6-7bba-4ec1-9faa-7d54f84d45c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelTrainer:\n",
    "    \"\"\"Class to handle model training for different NLP tasks\"\"\"\n",
    "    def __init__(self, models_dir='./models'):\n",
    "        self.models_dir = models_dir\n",
    "        os.makedirs(models_dir, exist_ok=True)\n",
    "\n",
    "    def train_classification_model(self, train_df, val_df, num_labels=2, model_name='distilbert-base-uncased',\n",
    "                                   epochs=3, batch_size=16, learning_rate=2e-5):\n",
    "        \"\"\"Train a text classification model\"\"\"\n",
    "        print(f\"Training classification model: {model_name}\")\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        # Initialize tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = AutoModelForSequenceClassification.from_pretrained(\n",
    "            model_name,\n",
    "            num_labels=num_labels\n",
    "        )\n",
    "        model.to(device)\n",
    "\n",
    "        # Create data loaders\n",
    "        train_dataset = TextClassificationDataset(\n",
    "            texts=train_df['text'].tolist(),\n",
    "            labels=train_df['label'].tolist(),\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        val_dataset = TextClassificationDataset(\n",
    "            texts=val_df['text'].tolist(),\n",
    "            labels=val_df['label'].tolist(),\n",
    "            tokenizer=tokenizer\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        # Prepare optimizer and scheduler\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "        total_steps = len(train_loader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        # Training loop\n",
    "        best_val_accuracy = 0\n",
    "        train_losses = []\n",
    "        val_metrics = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "\n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['label'].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "            print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "            # Validation\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            predictions = []\n",
    "            actual_labels = []\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for batch in tqdm(val_loader, desc=\"Validation\"):\n",
    "                    input_ids = batch['input_ids'].to(device)\n",
    "                    attention_mask = batch['attention_mask'].to(device)\n",
    "                    labels = batch['label'].to(device)\n",
    "\n",
    "                    outputs = model(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        labels=labels\n",
    "                    )\n",
    "\n",
    "                    loss = outputs.loss\n",
    "                    val_loss += loss.item()\n",
    "\n",
    "                    _, preds = torch.max(outputs.logits, dim=1)\n",
    "\n",
    "                    predictions.extend(preds.cpu().tolist())\n",
    "                    actual_labels.extend(labels.cpu().tolist())\n",
    "\n",
    "            avg_val_loss = val_loss / len(val_loader)\n",
    "            val_accuracy = accuracy_score(actual_labels, predictions)\n",
    "            precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "                actual_labels, predictions, average='weighted'\n",
    "            )\n",
    "\n",
    "            val_metrics.append({\n",
    "                'loss': avg_val_loss,\n",
    "                'accuracy': val_accuracy,\n",
    "                'precision': precision,\n",
    "                'recall': recall,\n",
    "                'f1': f1\n",
    "            })\n",
    "\n",
    "            print(f\"Validation Loss: {avg_val_loss:.4f}\")\n",
    "            print(f\"Accuracy: {val_accuracy:.4f}\")\n",
    "            print(f\"Precision: {precision:.4f}\")\n",
    "            print(f\"Recall: {recall:.4f}\")\n",
    "            print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "            # Save best model\n",
    "            if val_accuracy > best_val_accuracy:\n",
    "                best_val_accuracy = val_accuracy\n",
    "                print(f\"Saving best model with accuracy: {best_val_accuracy:.4f}\")\n",
    "\n",
    "                # Create directory for model\n",
    "                model_save_dir = f\"{self.models_dir}/classification\"\n",
    "                os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "                # Save model and tokenizer\n",
    "                model.save_pretrained(model_save_dir)\n",
    "                tokenizer.save_pretrained(model_save_dir)\n",
    "\n",
    "                # Save training metrics\n",
    "                metrics = {\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_metrics': val_metrics,\n",
    "                    'best_val_accuracy': best_val_accuracy,\n",
    "                    'epochs_trained': epoch + 1,\n",
    "                    'model_name': model_name,\n",
    "                    'num_labels': num_labels\n",
    "                }\n",
    "\n",
    "                with open(f\"{model_save_dir}/training_metrics.json\", 'w') as f:\n",
    "                    json.dump(metrics, f)\n",
    "\n",
    "        print(f\"Classification model training complete. Best accuracy: {best_val_accuracy:.4f}\")\n",
    "        return model_save_dir\n",
    "\n",
    "    def train_summarization_model(self, train_df, val_df, model_name='t5-small',\n",
    "                                  epochs=3, batch_size=8, learning_rate=5e-5):\n",
    "        \"\"\"Train a text summarization model\"\"\"\n",
    "        print(f\"Training summarization model: {model_name}\")\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        # Initialize tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # The original code used AutoModelForSeq2SeqLM, which is correct for summarization\n",
    "        model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "        model.to(device)\n",
    "\n",
    "        # For T5, we need to add a prefix\n",
    "        train_df['source_text'] = train_df['document'].apply(lambda x: f\"summarize: {x}\")\n",
    "        val_df['source_text'] = val_df['document'].apply(lambda x: f\"summarize: {x}\")\n",
    "\n",
    "        # Create data loaders\n",
    "        train_dataset = TextSeq2SeqDataset(\n",
    "            source_texts=train_df['source_text'].tolist(),\n",
    "            target_texts=train_df['summary'].tolist(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_source_len=512,\n",
    "            max_target_len=128\n",
    "        )\n",
    "\n",
    "        val_dataset = TextSeq2SeqDataset(\n",
    "            source_texts=val_df['source_text'].tolist(),\n",
    "            target_texts=val_df['summary'].tolist(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_source_len=512,\n",
    "            max_target_len=128\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        # Prepare optimizer and scheduler\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "        total_steps = len(train_loader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        # Training loop\n",
    "        best_val_rouge = 0\n",
    "        train_losses = []\n",
    "        val_metrics = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "\n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "            print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "            # Validation - evaluate with ROUGE\n",
    "            model.eval()\n",
    "            val_loss = 0\n",
    "            generated_summaries = []\n",
    "            reference_summaries = []\n",
    "\n",
    "            # Use a smaller subset for validation to save time\n",
    "            max_val_samples = min(100, len(val_df))\n",
    "            val_subset = val_df.iloc[:max_val_samples]\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for i, row in tqdm(val_subset.iterrows(), total=len(val_subset), desc=\"Validation\"):\n",
    "                    document = row['source_text']\n",
    "                    reference = row['summary']\n",
    "\n",
    "                    # Tokenize\n",
    "                    inputs = tokenizer(document, return_tensors=\"pt\", max_length=512, truncation=True)\n",
    "                    input_ids = inputs.input_ids.to(device)\n",
    "                    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "                    # Generate summary\n",
    "                    summary_ids = model.generate(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        max_length=128,\n",
    "                        min_length=30,\n",
    "                        no_repeat_ngram_size=3,\n",
    "                        early_stopping=True\n",
    "                    )\n",
    "\n",
    "                    summary = tokenizer.decode(summary_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                    generated_summaries.append(summary)\n",
    "                    reference_summaries.append(reference)\n",
    "\n",
    "            # Compute ROUGE scores\n",
    "            rouge = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
    "            rouge_scores = []\n",
    "\n",
    "            for pred, ref in zip(generated_summaries, reference_summaries):\n",
    "                score = rouge.score(pred, ref)\n",
    "                rouge_scores.append(score)\n",
    "\n",
    "            # Calculate average ROUGE scores\n",
    "            avg_rouge1 = np.mean([score['rouge1'].fmeasure for score in rouge_scores])\n",
    "            avg_rouge2 = np.mean([score['rouge2'].fmeasure for score in rouge_scores])\n",
    "            avg_rougeL = np.mean([score['rougeL'].fmeasure for score in rouge_scores])\n",
    "            avg_rouge = (avg_rouge1 + avg_rouge2 + avg_rougeL) / 3\n",
    "\n",
    "            val_metric = {\n",
    "                'rouge1': avg_rouge1,\n",
    "                'rouge2': avg_rouge2,\n",
    "                'rougeL': avg_rougeL,\n",
    "                'avg_rouge': avg_rouge\n",
    "            }\n",
    "\n",
    "            val_metrics.append(val_metric)\n",
    "\n",
    "            print(f\"ROUGE-1: {avg_rouge1:.4f}\")\n",
    "            print(f\"ROUGE-2: {avg_rouge2:.4f}\")\n",
    "            print(f\"ROUGE-L: {avg_rougeL:.4f}\")\n",
    "            print(f\"Avg ROUGE: {avg_rouge:.4f}\")\n",
    "\n",
    "            # Save best model based on average ROUGE score\n",
    "            if avg_rouge > best_val_rouge:\n",
    "                best_val_rouge = avg_rouge\n",
    "                print(f\"Saving best model with avg ROUGE: {best_val_rouge:.4f}\")\n",
    "\n",
    "                # Create directory for model\n",
    "                model_save_dir = f\"{self.models_dir}/summarization\"\n",
    "                os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "                # Save model and tokenizer\n",
    "                model.save_pretrained(model_save_dir)\n",
    "                tokenizer.save_pretrained(model_save_dir)\n",
    "\n",
    "                # Save training metrics\n",
    "                metrics = {\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_metrics': val_metrics,\n",
    "                    'best_val_rouge': best_val_rouge,\n",
    "                    'epochs_trained': epoch + 1,\n",
    "                    'model_name': model_name\n",
    "                }\n",
    "\n",
    "                with open(f\"{model_save_dir}/training_metrics.json\", 'w') as f:\n",
    "                    json.dump(metrics, f)\n",
    "\n",
    "        print(f\"Summarization model training complete. Best avg ROUGE: {best_val_rouge:.4f}\")\n",
    "        return model_save_dir\n",
    "\n",
    "    def train_translation_model(self, train_df, val_df, model_name='t5-small',\n",
    "                                source_lang='de', target_lang='en',\n",
    "                                epochs=3, batch_size=8, learning_rate=5e-5):\n",
    "        \"\"\"Train a machine translation model\"\"\"\n",
    "        print(f\"Training translation model: {model_name} ({source_lang} to {target_lang})\")\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        print(f\"Using device: {device}\")\n",
    "\n",
    "        # Initialize tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        model = T5ForConditionalGeneration.from_pretrained(model_name)\n",
    "        model.to(device)\n",
    "\n",
    "        # Prepare text with language prefix for T5\n",
    "        train_df['source_text'] = train_df.apply(\n",
    "            lambda row: f\"translate {source_lang} to {target_lang}: {row['source_text']}\", axis=1)\n",
    "        val_df['source_text'] = val_df.apply(\n",
    "            lambda row: f\"translate {source_lang} to {target_lang}: {row['source_text']}\", axis=1)\n",
    "\n",
    "        # Create data loaders\n",
    "        train_dataset = TextSeq2SeqDataset(\n",
    "            source_texts=train_df['source_text'].tolist(),\n",
    "            target_texts=train_df['target_text'].tolist(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_source_len=128,\n",
    "            max_target_len=128\n",
    "        )\n",
    "\n",
    "        val_dataset = TextSeq2SeqDataset(\n",
    "            source_texts=val_df['source_text'].tolist(),\n",
    "            target_texts=val_df['target_text'].tolist(),\n",
    "            tokenizer=tokenizer,\n",
    "            max_source_len=128,\n",
    "            max_target_len=128\n",
    "        )\n",
    "\n",
    "        train_loader = DataLoader(\n",
    "            train_dataset,\n",
    "            batch_size=batch_size,\n",
    "            shuffle=True\n",
    "        )\n",
    "\n",
    "        val_loader = DataLoader(\n",
    "            val_dataset,\n",
    "            batch_size=batch_size\n",
    "        )\n",
    "\n",
    "        # Prepare optimizer and scheduler\n",
    "        optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "        total_steps = len(train_loader) * epochs\n",
    "        scheduler = get_linear_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=0,\n",
    "            num_training_steps=total_steps\n",
    "        )\n",
    "\n",
    "        # Training loop\n",
    "        best_val_bleu = 0\n",
    "        train_losses = []\n",
    "        val_metrics = []\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            print(f\"Epoch {epoch + 1}/{epochs}\")\n",
    "\n",
    "            # Training\n",
    "            model.train()\n",
    "            train_loss = 0\n",
    "            progress_bar = tqdm(train_loader, desc=\"Training\")\n",
    "\n",
    "            for batch in progress_bar:\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                input_ids = batch['input_ids'].to(device)\n",
    "                attention_mask = batch['attention_mask'].to(device)\n",
    "                labels = batch['labels'].to(device)\n",
    "\n",
    "                outputs = model(\n",
    "                    input_ids=input_ids,\n",
    "                    attention_mask=attention_mask,\n",
    "                    labels=labels\n",
    "                )\n",
    "\n",
    "                loss = outputs.loss\n",
    "                train_loss += loss.item()\n",
    "\n",
    "                loss.backward()\n",
    "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "\n",
    "                progress_bar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "            avg_train_loss = train_loss / len(train_loader)\n",
    "            train_losses.append(avg_train_loss)\n",
    "            print(f\"Average training loss: {avg_train_loss:.4f}\")\n",
    "\n",
    "            # Validation - evaluate with BLEU score\n",
    "            model.eval()\n",
    "            translations = []\n",
    "            references = []\n",
    "\n",
    "            # Use a smaller subset for validation to save time\n",
    "            max_val_samples = min(100, len(val_df))\n",
    "            val_subset = val_df.iloc[:max_val_samples]\n",
    "\n",
    "            smooth = SmoothingFunction().method1\n",
    "\n",
    "            with torch.no_grad():\n",
    "                for i, row in tqdm(val_subset.iterrows(), total=len(val_subset), desc=\"Validation\"):\n",
    "                    source_text = row['source_text']\n",
    "                    target_text = row['target_text']\n",
    "\n",
    "                    # Tokenize\n",
    "                    inputs = tokenizer(source_text, return_tensors=\"pt\", max_length=128, truncation=True)\n",
    "                    input_ids = inputs.input_ids.to(device)\n",
    "                    attention_mask = inputs.attention_mask.to(device)\n",
    "\n",
    "                    # Generate translation\n",
    "                    output_ids = model.generate(\n",
    "                        input_ids=input_ids,\n",
    "                        attention_mask=attention_mask,\n",
    "                        max_length=128,\n",
    "                        num_beams=4,\n",
    "                        early_stopping=True\n",
    "                    )\n",
    "\n",
    "                    translation = tokenizer.decode(output_ids[0], skip_special_tokens=True)\n",
    "\n",
    "                    translations.append(translation)\n",
    "                    references.append(target_text)\n",
    "\n",
    "            # Compute BLEU scores\n",
    "            bleu_scores = []\n",
    "\n",
    "            for pred, ref in zip(translations, references):\n",
    "                pred_tokens = pred.split()\n",
    "                ref_tokens = [ref.split()]\n",
    "\n",
    "                try:\n",
    "                    score = sentence_bleu(\n",
    "                        ref_tokens,\n",
    "                        pred_tokens,\n",
    "                        smoothing_function=smooth\n",
    "                    )\n",
    "                    bleu_scores.append(score)\n",
    "                except Exception as e:\n",
    "                    print(f\"Error computing BLEU: {e}\")\n",
    "                    continue\n",
    "\n",
    "            avg_bleu = np.mean(bleu_scores) if bleu_scores else 0\n",
    "\n",
    "            val_metric = {\n",
    "                'bleu': avg_bleu\n",
    "            }\n",
    "\n",
    "            val_metrics.append(val_metric)\n",
    "\n",
    "            print(f\"BLEU Score: {avg_bleu:.4f}\")\n",
    "\n",
    "            # Save best model based on BLEU score\n",
    "            if avg_bleu > best_val_bleu:\n",
    "                best_val_bleu = avg_bleu\n",
    "                print(f\"Saving best model with BLEU: {best_val_bleu:.4f}\")\n",
    "\n",
    "                # Create directory for model\n",
    "                model_save_dir = f\"{self.models_dir}/translation\"\n",
    "                os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "                # Save model and tokenizer\n",
    "                model.save_pretrained(model_save_dir)\n",
    "                tokenizer.save_pretrained(model_save_dir)\n",
    "\n",
    "                # Save training metrics\n",
    "                metrics = {\n",
    "                    'train_losses': train_losses,\n",
    "                    'val_metrics': val_metrics,\n",
    "                    'best_val_bleu': best_val_bleu,\n",
    "                    'epochs_trained': epoch + 1,\n",
    "                    'model_name': model_name,\n",
    "                    'source_lang': source_lang,\n",
    "                    'target_lang': target_lang\n",
    "                }\n",
    "\n",
    "                with open(f\"{model_save_dir}/training_metrics.json\", 'w') as f:\n",
    "                    json.dump(metrics, f)\n",
    "\n",
    "        print(f\"Translation model training complete. Best BLEU: {best_val_bleu:.4f}\")\n",
    "        return model_save_dir\n",
    "\n",
    "    def setup_text_generation_model(self, model_name='gpt2', save_dir=None):\n",
    "        \"\"\"\n",
    "        Set up a pre-trained model for text generation\n",
    "        For generation, we'll use a pre-trained model directly\n",
    "        \"\"\"\n",
    "        print(f\"Setting up text generation model: {model_name}\")\n",
    "\n",
    "        # Initialize tokenizer and model\n",
    "        tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        # Use AutoModelForCausalLM for text generation models like GPT-2\n",
    "        model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "        # If save_dir is provided, save the model\n",
    "        if save_dir:\n",
    "            model_save_dir = f\"{self.models_dir}/generation\"\n",
    "            os.makedirs(model_save_dir, exist_ok=True)\n",
    "\n",
    "            model.save_pretrained(model_save_dir)\n",
    "            tokenizer.save_pretrained(model_save_dir)\n",
    "\n",
    "            # Save model info\n",
    "            model_info = {\n",
    "                'model_name': model_name\n",
    "            }\n",
    "\n",
    "            with open(f\"{model_save_dir}/model_info.json\", 'w') as f:\n",
    "                json.dump(model_info, f)\n",
    "\n",
    "            print(f\"Text generation model saved to {model_save_dir}\")\n",
    "            return model_save_dir\n",
    "        else:\n",
    "            return model_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "48c54ec7-5c30-4b8c-b761-3b961e9f554b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Setting Up Text Generation Model ===\n",
      "Setting up text generation model: gpt2\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6fca5d5342f41378728797a5044928f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a85532ca37f24d6e8f861922d02840f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "17f201965c934ff19c0bf072a618dd78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5038918a026848ae924629428ac559a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d313ebf698544f95a9ada468e9856111",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Unrecognized configuration class <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, Qwen2AudioConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 26\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m args\u001b[38;5;241m.\u001b[39mtask \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mall\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m=== Setting Up Text Generation Model ===\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 26\u001b[0m     trainer\u001b[38;5;241m.\u001b[39msetup_text_generation_model(save_dir\u001b[38;5;241m=\u001b[39margs\u001b[38;5;241m.\u001b[39mmodels_dir)\n\u001b[1;32m     28\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mTraining complete! Models saved to\u001b[39m\u001b[38;5;124m\"\u001b[39m, args\u001b[38;5;241m.\u001b[39mmodels_dir)\n",
      "Cell \u001b[0;32mIn[31], line 552\u001b[0m, in \u001b[0;36mModelTrainer.setup_text_generation_model\u001b[0;34m(self, model_name, save_dir)\u001b[0m\n\u001b[1;32m    550\u001b[0m \u001b[38;5;66;03m# Initialize tokenizer and model\u001b[39;00m\n\u001b[1;32m    551\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m AutoTokenizer\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[0;32m--> 552\u001b[0m model \u001b[38;5;241m=\u001b[39m AutoModelForSeq2SeqLM\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name)\n\u001b[1;32m    554\u001b[0m \u001b[38;5;66;03m# If save_dir is provided, save the model\u001b[39;00m\n\u001b[1;32m    555\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m save_dir:\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py:574\u001b[0m, in \u001b[0;36m_BaseAutoModelClass.from_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    570\u001b[0m         config \u001b[38;5;241m=\u001b[39m config\u001b[38;5;241m.\u001b[39mget_text_config()\n\u001b[1;32m    571\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model_class\u001b[38;5;241m.\u001b[39mfrom_pretrained(\n\u001b[1;32m    572\u001b[0m         pretrained_model_name_or_path, \u001b[38;5;241m*\u001b[39mmodel_args, config\u001b[38;5;241m=\u001b[39mconfig, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs\n\u001b[1;32m    573\u001b[0m     )\n\u001b[0;32m--> 574\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    575\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnrecognized configuration class \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mconfig\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m for this kind of AutoModel: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    576\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mModel type should be one of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(c\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mfor\u001b[39;00m\u001b[38;5;250m \u001b[39mc\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01min\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m_model_mapping\u001b[38;5;241m.\u001b[39mkeys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    577\u001b[0m )\n",
      "\u001b[0;31mValueError\u001b[0m: Unrecognized configuration class <class 'transformers.models.gpt2.configuration_gpt2.GPT2Config'> for this kind of AutoModel: AutoModelForSeq2SeqLM.\nModel type should be one of BartConfig, BigBirdPegasusConfig, BlenderbotConfig, BlenderbotSmallConfig, EncoderDecoderConfig, FSMTConfig, GPTSanJapaneseConfig, LEDConfig, LongT5Config, M2M100Config, MarianConfig, MBartConfig, MT5Config, MvpConfig, NllbMoeConfig, PegasusConfig, PegasusXConfig, PLBartConfig, ProphetNetConfig, Qwen2AudioConfig, SeamlessM4TConfig, SeamlessM4Tv2Config, SwitchTransformersConfig, T5Config, UMT5Config, XLMProphetNetConfig."
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    import argparse\n",
    "    import sys\n",
    "\n",
    "    # Check if running in Jupyter or similar environment\n",
    "    if 'ipykernel' in sys.modules:\n",
    "        # If running in Jupyter, use an empty list for args\n",
    "        args = argparse.Namespace(task='all', data_dir='./data', models_dir='./models', epochs=3)\n",
    "    else:\n",
    "        parser = argparse.ArgumentParser(description='Train NLP models')\n",
    "        parser.add_argument('--task', type=str, choices=['classification', 'summarization', 'translation', 'generation', 'all'],\n",
    "                            default='all', help='NLP task to train model for')\n",
    "        parser.add_argument('--data_dir', type=str, default='./data', help='Directory with preprocessed data')\n",
    "        parser.add_argument('--models_dir', type=str, default='./models', help='Directory to save trained models')\n",
    "        parser.add_argument('--epochs', type=int, default=3, help='Number of training epochs')\n",
    "\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    trainer = ModelTrainer(models_dir=args.models_dir)\n",
    "\n",
    "    # Load data based on task\n",
    "    if args.task == 'classification' or args.task == 'all':\n",
    "        print(\"\\n=== Training Classification Model ===\")\n",
    "        train_df = pd.read_csv(f\"{args.data_dir}/classification_train.csv\")\n",
    "        val_df = pd.read_csv(f\"{args.data_dir}/classification_val.csv\")\n",
    "        trainer.train_classification_model(train_df, val_df, epochs=args.epochs)\n",
    "\n",
    "    if args.task == 'summarization' or args.task == 'all':\n",
    "        print(\"\\n=== Training Summarization Model ===\")\n",
    "        train_df = pd.read_csv(f\"{args.data_dir}/summarization_train.csv\")\n",
    "        val_df = pd.read_csv(f\"{args.data_dir}/summarization_val.csv\")\n",
    "        trainer.train_summarization_model(train_df, val_df, epochs=args.epochs)\n",
    "\n",
    "    if args.task == 'translation' or args.task == 'all':\n",
    "        print(\"\\n=== Training Translation Model ===\")\n",
    "        train_df = pd.read_csv(f\"{args.data_dir}/translation_train.csv\")\n",
    "        val_df = pd.read_csv(f\"{args.data_dir}/translation_val.csv\")\n",
    "        trainer.train_translation_model(\n",
    "            train_df, val_df,\n",
    "            source_lang=train_df['source_lang'].iloc[0],\n",
    "            target_lang=train_df['target_lang'].iloc[0],\n",
    "            epochs=args.epochs\n",
    "        )\n",
    "\n",
    "    if args.task == 'generation' or args.task == 'all': # Corrected the condition here\n",
    "        print(\"\\n=== Setting Up Text Generation Model ===\")\n",
    "        trainer.setup_text_generation_model(save_dir=args.models_dir)\n",
    "\n",
    "    print(\"\\nTraining complete! Models saved to\", args.models_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "958c68bf-4161-44c5-a3f1-b0a5adaf2c54",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7399901d-4308-45f2-aadd-eb5622a8612f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
