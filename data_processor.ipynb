{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3ecd369d-bb11-4cd0-99d1-0873d814dd8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import re\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from sklearn.model_selection import train_test_split\n",
    "from datasets import load_dataset\n",
    "import os\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fc9d65c-5a08-494c-b5a3-af5d21310e10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary NLTK resources\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "nltk.download('wordnet', quiet=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8bba0c56-55d5-462b-8ea6-f545a6f2eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcessor:\n",
    "    \"\"\"\n",
    "    Class for data collection and preprocessing for multiple NLP tasks\n",
    "    \"\"\"\n",
    "    def __init__(self, cache_dir='./data'):\n",
    "        self.cache_dir = cache_dir\n",
    "        os.makedirs(cache_dir, exist_ok=True)\n",
    "\n",
    "    def load_classification_data(self, dataset_name='imdb', split=['train', 'test']):\n",
    "        \"\"\"\n",
    "        Load and preprocess data for text classification task\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Loading classification dataset: {dataset_name}\")\n",
    "\n",
    "            # Load dataset using Hugging Face datasets\n",
    "            dataset = load_dataset(dataset_name)\n",
    "\n",
    "            # Extract data\n",
    "            if dataset_name == 'imdb':\n",
    "                # IMDB dataset for sentiment analysis\n",
    "                train_data = dataset[\"train\"]\n",
    "                test_data = dataset[\"test\"]\n",
    "\n",
    "                # Check if data is empty\n",
    "                if not train_data or not test_data:\n",
    "                    raise ValueError(\"Loaded dataset is empty.\")\n",
    "\n",
    "                train_df = pd.DataFrame({\n",
    "                    'text': train_data['text'],\n",
    "                    'label': train_data['label']\n",
    "                })\n",
    "\n",
    "                test_df = pd.DataFrame({\n",
    "                    'text': test_data['text'],\n",
    "                    'label': test_data['label']\n",
    "                })\n",
    "\n",
    "                # Map numeric labels to string labels for clarity\n",
    "                train_df['sentiment'] = train_df['label'].map({0: 'negative', 1: 'positive'})\n",
    "                test_df['sentiment'] = test_df['label'].map({0: 'negative', 1: 'positive'})\n",
    "\n",
    "                # Prepare train/val/test split\n",
    "                val_df = train_df.sample(frac=0.1, random_state=42)\n",
    "                train_df = train_df.drop(val_df.index)\n",
    "\n",
    "                # Save processed data\n",
    "                train_df.to_csv(f\"{self.cache_dir}/classification_train.csv\", index=False)\n",
    "                val_df.to_csv(f\"{self.cache_dir}/classification_val.csv\", index=False)\n",
    "                test_df.to_csv(f\"{self.cache_dir}/classification_test.csv\", index=False)\n",
    "\n",
    "                print(f\"Classification data saved. Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "                return {\n",
    "                    'train': train_df,\n",
    "                    'val': val_df,\n",
    "                    'test': test_df,\n",
    "                    'label_map': {0: 'negative', 1: 'positive'},\n",
    "                    'num_labels': 2\n",
    "                }\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading classification dataset: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_summarization_data(self, dataset_name='cnn_dailymail', version='3.0.0', split=['train', 'validation', 'test']):\n",
    "        \"\"\"\n",
    "        Load and preprocess data for text summarization task\n",
    "        \"\"\"\n",
    "        try:\n",
    "            print(f\"Loading summarization dataset: {dataset_name}\")\n",
    "\n",
    "            # Load dataset using Hugging Face datasets\n",
    "            dataset = load_dataset(dataset_name, version)\n",
    "\n",
    "            # Extract samples from each split\n",
    "            train_samples = dataset['train'].select(range(min(1000, len(dataset['train']))))\n",
    "            val_samples = dataset['validation'].select(range(min(200, len(dataset['validation']))))\n",
    "            test_samples = dataset['test'].select(range(min(200, len(dataset['test']))))\n",
    "\n",
    "            # Convert to DataFrames\n",
    "            train_df = pd.DataFrame({\n",
    "                'document': train_samples['article'],\n",
    "                'summary': train_samples['highlights']\n",
    "            })\n",
    "\n",
    "            val_df = pd.DataFrame({\n",
    "                'document': val_samples['article'],\n",
    "                'summary': val_samples['highlights']\n",
    "            })\n",
    "\n",
    "            test_df = pd.DataFrame({\n",
    "                'document': test_samples['article'],\n",
    "                'summary': test_samples['highlights']\n",
    "            })\n",
    "\n",
    "            # Save processed data\n",
    "            train_df.to_csv(f\"{self.cache_dir}/summarization_train.csv\", index=False)\n",
    "            val_df.to_csv(f\"{self.cache_dir}/summarization_val.csv\", index=False)\n",
    "            test_df.to_csv(f\"{self.cache_dir}/summarization_test.csv\", index=False)\n",
    "\n",
    "            print(f\"Summarization data saved. Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "            return {\n",
    "                'train': train_df,\n",
    "                'val': val_df,\n",
    "                'test': test_df\n",
    "            }\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading summarization dataset: {e}\")\n",
    "            return None\n",
    "\n",
    "    def load_translation_data(self, dataset_name='wmt16', language_pair='de-en', split=['train', 'validation', 'test']):\n",
    "        \"\"\"\n",
    "        Load and preprocess data for machine translation task\n",
    "        \"\"\"\n",
    "        print(f\"Loading translation dataset: {dataset_name} ({language_pair})\")\n",
    "\n",
    "        # Load dataset using Hugging Face datasets\n",
    "        try:\n",
    "            dataset = load_dataset(dataset_name, language_pair)\n",
    "\n",
    "            # Extract language codes\n",
    "            source_lang, target_lang = language_pair.split('-')\n",
    "\n",
    "            # Extract samples from each split (limiting size to make it manageable)\n",
    "            train_samples = dataset['train'].select(range(min(1000, len(dataset['train']))))\n",
    "            val_samples = dataset['validation'].select(range(min(200, len(dataset['validation']))))\n",
    "            test_samples = dataset['test'].select(range(min(200, len(dataset['test']))))\n",
    "\n",
    "            # Convert to DataFrames\n",
    "            train_df = pd.DataFrame({\n",
    "                'source_text': [item['translation'][source_lang] for item in train_samples],\n",
    "                'target_text': [item['translation'][target_lang] for item in train_samples],\n",
    "                'source_lang': source_lang,\n",
    "                'target_lang': target_lang\n",
    "            })\n",
    "\n",
    "            val_df = pd.DataFrame({\n",
    "                'source_text': [item['translation'][source_lang] for item in val_samples],\n",
    "                'target_text': [item['translation'][target_lang] for item in val_samples],\n",
    "                'source_lang': source_lang,\n",
    "                'target_lang': target_lang\n",
    "            })\n",
    "\n",
    "            test_df = pd.DataFrame({\n",
    "                'source_text': [item['translation'][source_lang] for item in test_samples],\n",
    "                'target_text': [item['translation'][target_lang] for item in test_samples],\n",
    "                'source_lang': source_lang,\n",
    "                'target_lang': target_lang\n",
    "            })\n",
    "\n",
    "            # Save processed data\n",
    "            train_df.to_csv(f\"{self.cache_dir}/translation_train.csv\", index=False)\n",
    "            val_df.to_csv(f\"{self.cache_dir}/translation_val.csv\", index=False)\n",
    "            test_df.to_csv(f\"{self.cache_dir}/translation_test.csv\", index=False)\n",
    "\n",
    "            print(f\"Translation data saved. Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "            return {\n",
    "                'train': train_df,\n",
    "                'val': val_df,\n",
    "                'test': test_df,\n",
    "                'source_lang': source_lang,\n",
    "                'target_lang': target_lang\n",
    "            }\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading translation dataset: {e}\")\n",
    "            # Create a small demo dataset if loading fails\n",
    "            print(\"Creating a small demo translation dataset instead\")\n",
    "\n",
    "            # Demo German-English pairs\n",
    "            de_en_pairs = [\n",
    "                (\"Hallo, wie geht es dir?\", \"Hello, how are you?\"),\n",
    "                (\"Ich liebe Programmierung.\", \"I love programming.\"),\n",
    "                (\"Das Wetter ist heute schön.\", \"The weather is nice today.\"),\n",
    "                (\"Können Sie mir bitte helfen?\", \"Can you please help me?\"),\n",
    "                (\"Vielen Dank für Ihre Hilfe.\", \"Thank you very much for your help.\")\n",
    "            ]\n",
    "\n",
    "            # Split into train/val/test\n",
    "            train_pairs = de_en_pairs[:3]\n",
    "            val_pairs = de_en_pairs[3:4]\n",
    "            test_pairs = de_en_pairs[4:]\n",
    "\n",
    "            # Create DataFrames\n",
    "            train_df = pd.DataFrame({\n",
    "                'source_text': [pair[0] for pair in train_pairs],\n",
    "                'target_text': [pair[1] for pair in train_pairs],\n",
    "                'source_lang': 'de',\n",
    "                'target_lang': 'en'\n",
    "            })\n",
    "\n",
    "            val_df = pd.DataFrame({\n",
    "                'source_text': [pair[0] for pair in val_pairs],\n",
    "                'target_text': [pair[1] for pair in val_pairs],\n",
    "                'source_lang': 'de',\n",
    "                'target_lang': 'en'\n",
    "            })\n",
    "\n",
    "            test_df = pd.DataFrame({\n",
    "                'source_text': [pair[0] for pair in test_pairs],\n",
    "                'target_text': [pair[1] for pair in test_pairs],\n",
    "                'source_lang': 'de',\n",
    "                'target_lang': 'en'\n",
    "            })\n",
    "\n",
    "            # Save processed data\n",
    "            train_df.to_csv(f\"{self.cache_dir}/translation_train.csv\", index=False)\n",
    "            val_df.to_csv(f\"{self.cache_dir}/translation_val.csv\", index=False)\n",
    "            test_df.to_csv(f\"{self.cache_dir}/translation_test.csv\", index=False)\n",
    "\n",
    "            print(f\"Demo translation data saved. Train: {len(train_df)}, Val: {len(val_df)}, Test: {len(test_df)}\")\n",
    "\n",
    "            return {\n",
    "                'train': train_df,\n",
    "                'val': val_df,\n",
    "                'test': test_df,\n",
    "                'source_lang': 'de',\n",
    "                'target_lang': 'en'\n",
    "            }\n",
    "\n",
    "    def setup_text_generation_data(self):\n",
    "        \"\"\"\n",
    "        For text generation, we'll use a prompt-based approach\n",
    "        This creates a small set of example prompts\n",
    "        \"\"\"\n",
    "        print(\"Setting up text generation examples\")\n",
    "\n",
    "        examples = [\n",
    "            {\"prompt\": \"Write a short story about a robot learning to paint\", \"category\": \"creative\"},\n",
    "            {\"prompt\": \"Explain quantum computing to a 10-year-old\", \"category\": \"educational\"},\n",
    "            {\"prompt\": \"Write a professional email requesting a meeting\", \"category\": \"business\"},\n",
    "            {\"prompt\": \"Create a recipe for chocolate chip cookies\", \"category\": \"cooking\"},\n",
    "            {\"prompt\": \"Write a product description for a new smartphone\", \"category\": \"marketing\"}\n",
    "        ]\n",
    "\n",
    "        # Save example prompts\n",
    "        df = pd.DataFrame(examples)\n",
    "        df.to_csv(f\"{self.cache_dir}/generation_examples.csv\", index=False)\n",
    "\n",
    "        print(f\"Text generation examples saved: {len(df)} examples\")\n",
    "\n",
    "        return df\n",
    "\n",
    "    def preprocess_text(self, text, task='classification'):\n",
    "        \"\"\"\n",
    "        Preprocess text based on the target task\n",
    "        \"\"\"\n",
    "        if task == 'classification':\n",
    "            # Convert to lowercase\n",
    "            text = str(text).lower()\n",
    "\n",
    "            # Remove special characters, numbers, and extra whitespace\n",
    "            text = re.sub(r'[^a-zA-Z\\s]', '', text)\n",
    "            text = re.sub(r'\\s+', ' ', text).strip()\n",
    "\n",
    "            # Tokenize and remove stopwords\n",
    "            stop_words = set(stopwords.words('english'))\n",
    "            tokens = word_tokenize(text)\n",
    "            tokens = [word for word in tokens if word not in stop_words]\n",
    "\n",
    "            # Rejoin into a single string\n",
    "            processed_text = ' '.join(tokens)\n",
    "\n",
    "            return processed_text\n",
    "\n",
    "        elif task == 'summarization':\n",
    "            # For summarization, we need to preserve more of the original structure\n",
    "            # Just clean up and normalize\n",
    "            text = str(text).strip()\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            return text\n",
    "\n",
    "        elif task == 'translation':\n",
    "            # For translation, minimal preprocessing to preserve meaning\n",
    "            text = str(text).strip()\n",
    "            text = re.sub(r'\\s+', ' ', text)\n",
    "            return text\n",
    "\n",
    "        elif task == 'generation':\n",
    "            # For generation, clean but keep structure\n",
    "            text = str(text).strip()\n",
    "            return text\n",
    "\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown task: {task}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bc55d5b-4b0b-4dd9-86a8-16efc06565b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading classification dataset: imdb\n",
      "Classification data saved. Train: 22500, Val: 2500, Test: 25000\n",
      "Loading summarization dataset: cnn_dailymail\n",
      "Summarization data saved. Train: 1000, Val: 200, Test: 200\n",
      "Loading translation dataset: wmt16 (de-en)\n",
      "Translation data saved. Train: 1000, Val: 200, Test: 200\n",
      "Setting up text generation examples\n",
      "Text generation examples saved: 5 examples\n",
      "Data collection and preprocessing complete for all tasks!\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # Example usage\n",
    "    processor = DataProcessor()\n",
    "\n",
    "    # Load datasets for each task\n",
    "    classification_data = processor.load_classification_data()\n",
    "    summarization_data = processor.load_summarization_data()\n",
    "    translation_data = processor.load_translation_data()\n",
    "    generation_examples = processor.setup_text_generation_data()\n",
    "\n",
    "    print(\"Data collection and preprocessing complete for all tasks!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92b8c7c2-b2cb-4ce4-8ef4-bc4fe673a686",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
